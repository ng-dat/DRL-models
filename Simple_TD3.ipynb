{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFVyw11gTS0PHZf8XKBu8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nguyencongdat1997/RL.TryOut/blob/developments-ppo/Simple_TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXq4ga96ssF3"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsWq1yvNJN51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1632e8f0-fa85-4284-f409-2be253329faf"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "# import gym\n",
        "# env = gym.make(\"LunarLander-v2\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 12.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWnUGUsysLA9"
      },
      "source": [
        "import gym\n",
        "import random"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYrcJIslHH1A",
        "outputId": "85a74d88-e7f0-4f96-8b70-3d7d27052889"
      },
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "observation_space = env.observation_space.shape\n",
        "action_space = env.action_space"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om8yPoDZHLeS",
        "outputId": "cf9936ab-a2c8-4aa9-8467-605e69aaedf0"
      },
      "source": [
        "print(action_space)\n",
        "sample_action = env.action_space.sample()\n",
        "print(sample_action)\n",
        "print(observation_space)\n",
        "state = env.reset()\n",
        "print(state)\n",
        "state, reward, done, info = env.step(sample_action)\n",
        "print(state, reward, done, info)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(-1.0, 1.0, (4,), float32)\n",
            "[-0.20892707  0.11706837  0.6628375   0.53771186]\n",
            "(24,)\n",
            "[ 2.74599786e-03  9.57680924e-06 -1.24876454e-03 -1.60000646e-02\n",
            "  9.24573764e-02  2.89892987e-03  8.59824702e-01 -9.02140280e-04\n",
            "  1.00000000e+00  3.27833407e-02  2.89881648e-03  8.53548557e-01\n",
            " -1.97249915e-03  1.00000000e+00  4.40813392e-01  4.45819497e-01\n",
            "  4.61422116e-01  4.89549488e-01  5.34102023e-01  6.02460206e-01\n",
            "  7.09147871e-01  8.85930538e-01  1.00000000e+00  1.00000000e+00]\n",
            "[ 3.04660702e-04 -1.17167556e-02  4.73519862e-03  1.74500620e-02\n",
            " -2.70614445e-01 -5.17780602e-01  1.45148164e+00  7.85511176e-01\n",
            "  1.00000000e+00  2.87180662e-01 -3.16510558e-01  1.91316724e-01\n",
            "  5.21972060e-01  1.00000000e+00  4.55141306e-01  4.60310131e-01\n",
            "  4.76419896e-01  5.05461514e-01  5.51462173e-01  6.22042179e-01\n",
            "  7.32197583e-01  9.14726317e-01  1.00000000e+00  1.00000000e+00] -0.02945383042485261 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3JUBB2DPoZu",
        "outputId": "dfee7822-2cea-4c5d-de6e-afedb93bf71c"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "episodes = 3\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    steps = 0\n",
        "    while not done:\n",
        "        # env.render()\n",
        "        mu = np.random.normal(scale=env.action_space.high[0], size=(env.action_space.shape[0]))\n",
        "        clipped_mu = tf.clip_by_value(mu, env.action_space.low[0], env.action_space.high[0])\n",
        "        action = clipped_mu\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        steps += 1\n",
        "    print('Episode:{} Steps:{} Score:{}'.format(episode, steps, score))\n",
        "env.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:1 Steps:61 Score:-119.17519531884324\n",
            "Episode:2 Steps:91 Score:-103.2128391900563\n",
            "Episode:3 Steps:58 Score:-117.55205544809183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuCo8r734nQb"
      },
      "source": [
        "# TD3 tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNaQ7TEY4nQd"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w8hVyanBxbsB",
        "outputId": "456e7ec1-9a36-4318-e112-0ecbb58444ba"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__ #2.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAUpAPJTvk4r"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmrkGO-s4nQe"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4iv7cxvvvoV"
      },
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_counter = 0\n",
        "\n",
        "        self.states = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
        "        self.next_states = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
        "        self.rewards = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.actions = np.zeros((self.mem_size, n_actions), dtype=np.float32)\n",
        "        self.done = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "    def store_step(self, state, action, reward, next_state, done):\n",
        "        index = self.mem_counter % self.mem_size\n",
        "        self.states[index] = state\n",
        "        self.next_states[index] = next_state\n",
        "        self.actions[index] = action\n",
        "        self.rewards[index] = reward\n",
        "        self.done[index] = done\n",
        "        self.mem_counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        # if batch_size > self.mem_counter:\n",
        "        #   return [], [], [], [], []\n",
        "        max_mem = min(self.mem_counter, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.states[batch]\n",
        "        next_states = self.next_states[batch]\n",
        "        rewards = self.rewards[batch]\n",
        "        actions = self.actions[batch]\n",
        "        dones = self.done[batch]\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOwvvQiI4nQg"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlcesD9JwtU1"
      },
      "source": [
        "class Critic(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        l1_dims = 400 # As tested in the paper\n",
        "        l2_dims = 300 # As tested in the paper\n",
        "\n",
        "        self.fc1 = Dense(l1_dims, activation='relu') #input_shape  = state, action\n",
        "        self.fc2 = Dense(l2_dims, activation='relu')\n",
        "        self.q = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, state, action):\n",
        "        input = tf.concat([state, action], axis=1)\n",
        "        x = self.fc1(input)\n",
        "        x = self.fc2(x)\n",
        "        q = self.q(x)\n",
        "        return q"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utvG10nPyqeZ"
      },
      "source": [
        "class Actor(keras.Model):\n",
        "    def __init__(self, n_actions):\n",
        "        super(Actor, self).__init__()\n",
        "        l1_dims = 400 # As tested in the paper\n",
        "        l2_dims = 300 # As tested in the paper\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.fc1 = Dense(l1_dims, activation='relu') #input_shape  = state\n",
        "        self.fc2 = Dense(l2_dims, activation='relu')\n",
        "        self.mu = Dense(self.n_actions, activation='tanh')\n",
        "\n",
        "        #self.max_action = max_action\n",
        "\n",
        "    def call(self, state):        \n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        mu = self.mu(x)\n",
        "        # mu *= self.max_action\n",
        "        return mu"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWe_oqBC4nQh"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp1pRh-zz86Z"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, observation_shape, n_actions, max_action, min_action, \n",
        "                 actor_learning_rate=0.001, critic_learning_rate=0.001, tau=0.005, gamma=0.99, \n",
        "                 delayed_policy_update_interval=2, exploration_noise_std=0.1, \n",
        "                 smoothing_noise_std=0.2, smoothing_noise_limit=0.5,\n",
        "                 mem_size=1000000, batch_size=100):\n",
        "        self.n_actions = n_actions\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.gamma = gamma        \n",
        "        self.tau = tau        \n",
        "        self.delayed_policy_update_interval = delayed_policy_update_interval\n",
        "        self.exploration_noise_std = exploration_noise_std\n",
        "        self.smoothing_noise_std = smoothing_noise_std\n",
        "        self.smoothing_noise_limit = smoothing_noise_limit\n",
        "        self.batch_size = batch_size               \n",
        "\n",
        "        self.learned_step_counter = 0        \n",
        "        self.memory = ReplayBuffer(mem_size, observation_shape, n_actions)\n",
        "        self.actor = Actor(n_actions)\n",
        "        self.critic1 = Critic()\n",
        "        self.critic2 = Critic()\n",
        "        self.target_actor = Actor(n_actions)\n",
        "        self.target_critic1 = Critic()\n",
        "        self.target_critic2 = Critic()\n",
        "\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=actor_learning_rate), loss='mean')\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=actor_learning_rate), loss='mean')\n",
        "        self.critic1.compile(optimizer=Adam(learning_rate=critic_learning_rate), loss='mean_squared_error')\n",
        "        self.critic2.compile(optimizer=Adam(learning_rate=critic_learning_rate), loss='mean_squared_error')\n",
        "        self.target_critic1.compile(optimizer=Adam(learning_rate=critic_learning_rate), loss='mean_squared_error')\n",
        "        self.target_critic2.compile(optimizer=Adam(learning_rate=critic_learning_rate), loss='mean_squared_error')\n",
        "        \n",
        "        self.soft_update_target_network_parameters(tau=1)    \n",
        "    \n",
        "    def choose_action(self, observation, explorating=False):\n",
        "        # if self.time_step < self.warmup:\n",
        "        #     mu = np.random.normal(scale=self.exploration_noise_std, size=(self.n_actions)) #size=(self.n_actions,)\n",
        "        # else:\n",
        "            state = np.array([observation])\n",
        "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "            mu = self.actor(state)[0]\n",
        "            if explorating:                \n",
        "                mu = mu + np.random.normal(scale=self.exploration_noise_std)\n",
        "                mu = tf.clip_by_value(mu, self.min_action, self.max_action)\n",
        "            return mu\n",
        "    \n",
        "    def store_step(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_step(state, action, reward, next_state, done)\n",
        "       \n",
        "    def run_warmup(self, env, warmup):\n",
        "        print('Run warmup ----------')       \n",
        "        steps = 0        \n",
        "        while steps < warmup:\n",
        "            done = False\n",
        "            observation = env.reset()\n",
        "            while not done:\n",
        "                steps += 1\n",
        "                mu = np.random.normal(scale=self.max_action, size=(self.n_actions))\n",
        "                action = clipped_mu\n",
        "                next_observation, reward, done, info = env.step(action)\n",
        "                self.store_step(observation, action, reward, next_observation, done)\n",
        "                observation = next_observation                  \n",
        "\n",
        "    def train(self, env, n_games, warmup=1000):\n",
        "        if warmup > 0:\n",
        "            self.run_warmup(env, warmup)\n",
        "\n",
        "        print('Start training ----------')  \n",
        "        best_score = env.reward_range[0]\n",
        "        scores = []\n",
        "        time_steps = 0\n",
        "        for i in range(n_games):\n",
        "            done = False\n",
        "            score = 0\n",
        "            observation = env.reset()\n",
        "            while not done:\n",
        "                time_steps += 1\n",
        "                action = self.choose_action(observation, explorating=True)\n",
        "                next_observation, reward, done, info = env.step(action)\n",
        "                score += reward\n",
        "                self.store_step(observation, action, reward, next_observation, done)\n",
        "                self.learn()\n",
        "                observation = next_observation\n",
        "                self.learn()\n",
        "\n",
        "            scores.append(score)\n",
        "            avg_score = np.mean(scores[-10:])\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                if best_score > 250:\n",
        "                    self.save_models(train_dir='.')\n",
        "            print('Episode', i, '- trained steps', time_steps, '- score %.1f'%score, '- avg_score %.1f ' % avg_score)\n",
        "        print('End training ----------')  \n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape: # <- 2 networks update\n",
        "            target_actions = self.target_actor(next_states)\n",
        "            smoothing_noise = tf.clip_by_value(np.random.normal(scale=self.smoothing_noise_std), \n",
        "                                    -1*self.smoothing_noise_limit, self.smoothing_noise_limit)            \n",
        "            target_actions = target_actions + smoothing_noise\n",
        "            target_actions = tf.clip_by_value(target_actions, self.min_action, self.max_action)\n",
        "\n",
        "            next_q1 = self.target_critic1(next_states, target_actions)\n",
        "            next_q1 = tf.squeeze(next_q1, 1) # [batch_size, 1] -> [batch_size]\n",
        "            next_q2 = self.target_critic2(next_states, target_actions)\n",
        "            next_q2 = tf.squeeze(next_q2, 1)\n",
        "            next_q = tf.math.minimum(next_q1, next_q2)\n",
        "\n",
        "            q1 = self.critic1(states, actions)\n",
        "            q1 = tf.squeeze(q1, 1) \n",
        "            q2 = self.critic2(states, actions)\n",
        "            q2 = tf.squeeze(q2, 1) \n",
        "            \n",
        "            target = rewards + self.gamma*next_q*(1-dones)\n",
        "\n",
        "            critic1_loss = keras.losses.MSE(target, q1)\n",
        "            critic2_loss = keras.losses.MSE(target, q2)\n",
        "        critic1_gradient = tape.gradient(critic1_loss, self.critic1.trainable_variables)\n",
        "        critic2_gradient = tape.gradient(critic2_loss, self.critic2.trainable_variables)\n",
        "        self.critic1.optimizer.apply_gradients(zip(critic1_gradient, self.critic1.trainable_variables))\n",
        "        self.critic2.optimizer.apply_gradients(zip(critic2_gradient, self.critic2.trainable_variables))\n",
        "\n",
        "        self.learned_step_counter += 1\n",
        "\n",
        "        if self.learned_step_counter+1 % self.delayed_policy_update_interval == 0:\n",
        "            with tf.GradientTape() as tape:\n",
        "                new_actions = self.actor(states)\n",
        "                q = self.critic1(states, new_actions)\n",
        "                actor_loss =  -1 * tf.math.reduce_mean(q)\n",
        "            actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "            self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
        "\n",
        "            self.soft_update_target_network_parameters(tau=self.tau)\n",
        "\n",
        "    def soft_update_target_network_parameters(self, tau):\n",
        "        weights = []\n",
        "        targets = self.target_actor.weights\n",
        "        for i, weight in enumerate(self.actor.weights):\n",
        "            weights.append(weight*tau + targets[i]*(1-tau))\n",
        "        self.target_actor.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic1.weights\n",
        "        for i, weight in enumerate(self.critic1.weights):\n",
        "            weights.append(weight*tau + targets[i]*(1-tau))\n",
        "        self.target_critic1.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic2.weights\n",
        "        for i, weight in enumerate(self.critic2.weights):\n",
        "            weights.append(weight*tau + targets[i]*(1-tau))\n",
        "        self.target_critic2.set_weights(weights)\n",
        "\n",
        "    def save_models(self, train_dir):\n",
        "        pass\n",
        "        # train_dir = train_dir + '/td3_' + str(self.learned_step_counter) \n",
        "        # self.actor.save_weights(train-dir+'/actor/model')\n",
        "        # self.critic1.save_weights(train-dir+'/critic1/model')\n",
        "        # self.critic2.save_weights(train-dir+'/critic2/model')\n",
        "        # self.target_actor.save_weights(train-dir+'/target_actor/model')\n",
        "        # self.target_critic1.save_weights(train-dir+'/target_critic1/model')\n",
        "        # self.target_critic2.save_weights(train-dir+'/target_critic2/model')\n",
        "\n",
        "    def load_models(self, train_dir, learned_steps = 100):\n",
        "        pass\n",
        "        # train_dir = train_dir + '/td3_' + str(learned_steps) \n",
        "        # self.actor.load_weights(train-dir+'/actor/model')\n",
        "        # self.critic1.load_weights(train-dir+'/critic1/model')\n",
        "        # self.critic2.load_weights(train-dir+'/critic2/model')\n",
        "        # self.target_actor.load_weights(train-dir+'/target_actor/model')\n",
        "        # self.target_critic1.load_weights(train-dir+'/target_critic1/model')\n",
        "        # self.target_critic2.load_weights(train-dir+'/target_critic2/model')\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t66JBkJA4nQi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssk7wsKHGRV7"
      },
      "source": [
        "agent = Agent(observation_shape=env.observation_space.shape, \n",
        "              n_actions=env.action_space.shape[0], \n",
        "              max_action=env.action_space.high[0], \n",
        "              min_action=env.action_space.low[0],\n",
        "              critic_learning_rate=0.01, actor_learning_rate=0.01, tau=0.1)\n",
        "agent.train(env, n_games=1000, warmup=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmphIwVZ4nQi"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFNi_q964nQj"
      },
      "source": [
        "episodes = 3\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    steps = 0\n",
        "    while not done:\n",
        "        # env.render()        \n",
        "        action = agent.choose_action(state)\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        steps += 1\n",
        "    print('Episode:{} Steps:{} Score:{}'.format(episode, steps, score))\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDyHBWk0svdH"
      },
      "source": [
        "state = env.reset()\n",
        "action = agent.choose_action(state)\n",
        "action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SayFKSlAslxA"
      },
      "source": [
        "# TD3 pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou2KweMRslxC"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYZiwUdUslxC"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m94HBc8jslxD"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvRGYBbuslxE"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYia-ZYRslxE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8HstlOrslxF"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UB6REOAslxF"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}