{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple PPO - single processing, discrete action space, decoupled actor critic networks, no entropy bonus .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LuCo8r734nQb"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbG3BQzpm50wRr9jRYnFoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nguyencongdat1997/RL.TryOut/blob/developments-ppo/Simple_PPO_single_processing%2C_discrete_action_space%2C_decoupled_actor_critic_networks%2C_no_entropy_bonus_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNluuN1U4GLh"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHK3Ct0DIt_I"
      },
      "source": [
        "import gym\n",
        "import random"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3bkpQUoIugB"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "observation_shape = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "action_space = [x for x in range(actions)]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w4x103vI3Lh",
        "outputId": "8eada414-d72b-4014-ae4c-90e19d5c7dde"
      },
      "source": [
        "print(n_actions)\n",
        "sample_action = env.action_space.sample()\n",
        "print(sample_action)\n",
        "print(observation_shape)\n",
        "state = env.reset()\n",
        "print(state)\n",
        "state, reward, done, info = env.step(sample_action)\n",
        "print(state, reward, done, info)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "4\n",
            "[-0.04132161 -0.02188737  0.0433473  -0.0051939 ]\n",
            "[-0.04175936  0.17258698  0.04324342 -0.28389129] 1.0 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0pWmN23I4YH",
        "outputId": "a874ec22-c1a5-4526-b8e4-e6fbfa48d522"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = random.choice(action_space)\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "env.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:1 Score:21.0\n",
            "Episode:2 Score:12.0\n",
            "Episode:3 Score:41.0\n",
            "Episode:4 Score:22.0\n",
            "Episode:5 Score:21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmjUcC8K4NKY"
      },
      "source": [
        "# PPO torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu2IUnPw4Pvo"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sRwPpdU6o83"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiVkWBb64RN4"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPFttn5D69KX"
      },
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "    def store_step(self, state, action, probs, value, reward, next_state, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(value)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "                np.array(self.actions),\\\n",
        "                np.array(self.probs),\\\n",
        "                np.array(self.vals),\\\n",
        "                np.array(self.rewards),\\\n",
        "                np.array(self.dones),\\\n",
        "                batches\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.vals = []\n",
        "    "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5D47TGH4V-v"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJJy-JLf7KZo"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, n_actions, input_dims, learning_rate):        \n",
        "        super(Actor, self).__init__()\n",
        "        fc1_dims = 256\n",
        "        fc2_dims = 256\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(*input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims, fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc2_dims, n_actions),\n",
        "            nn.Softmax(dim= -1),\n",
        "        )        \n",
        "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        distribution = self.actor(state)\n",
        "        distribution = Categorical(distribution)\n",
        "        return distribution\n",
        "\n",
        "    def save_model(self, checkpoint_dir='./trained_models/ppo/torch'):\n",
        "        file_name = checkpoint_dir + '/actor/m'\n",
        "        T.save(self.state_dict(), file_name)\n",
        "    \n",
        "    def load_model(self, checkpoint_dir='./trained_models/ppo/torch'):\n",
        "        file_name = checkpoint_dir + '/actor/m'\n",
        "        self.load_state_dict(T.load(file_name))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8JkDEvz9T_V"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dims, learning_rate):        \n",
        "        super(Critic, self).__init__()\n",
        "        fc1_dims = 256\n",
        "        fc2_dims = 256\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(*input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims, fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc2_dims, 1)\n",
        "        )        \n",
        "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value\n",
        "\n",
        "    def save_model(self, checkpoint_dir='./trained_models/ppo/torch'):\n",
        "        file_name = checkpoint_dir + '/critic/m'\n",
        "        T.save(self.state_dict(), file_name)\n",
        "    \n",
        "    def load_model(self, checkpoint_dir='./trained_models/ppo/torch'):\n",
        "        file_name = checkpoint_dir + '/critic/m'\n",
        "        self.load_state_dict(T.load(file_name))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5YAhyPt4eLX"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rSgPoAN-iTH"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, n_actions, input_dims, gamma=0.99, actor_learning_rate=0.0003, critic_learning_rate=0.0003,\n",
        "               policy_clip=0.2, learn_batch_size=64, learn_epochs=10, gae_lambda=0.95, mem_size=100000):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.learn_epochs = learn_epochs\n",
        "        self.learn_batch_size = learn_batch_size\n",
        "        self.gae_lambda= gae_lambda\n",
        "\n",
        "        self.actor = Actor(n_actions, input_dims, actor_learning_rate)\n",
        "        self.critic = Critic(input_dims, critic_learning_rate)\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        \n",
        "    def store_step(self, state, action, probs, value, reward, next_state, done):\n",
        "        self.memory.store_step(state, action, probs, value, reward, next_state, done) \n",
        "\n",
        "    def save_models(self, checkpoint_dir='./trained_models/ppo/torch'):\n",
        "        self.actor.save_model(checkpoint_dir)\n",
        "        self.critic.save_model(checkpoint_dir)\n",
        "    \n",
        "    def load_models(self, checkpoint_dir='./trained_models/ppo/torch'):\n",
        "        self.actor.load_model(checkpoint_dir)\n",
        "        self.critic.load_model(checkpoint_dir)\n",
        "    \n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()        \n",
        "\n",
        "        prob = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return action, prob, value\n",
        "    \n",
        "    def learn(self):\n",
        "        for _ in range(self.learn_epochs):\n",
        "            states, actions, old_probs, values, rewards, dones, batches = self.memory.sample_buffer(self.learn_batch_size)\n",
        "            advantages = np.zeros(len(rewards), dtype=np.float32)\n",
        "            for t in range(len(rewards)-1):\n",
        "                discount = 1\n",
        "                advantage_t = 0\n",
        "                for k in range(t, len(rewards) -1):\n",
        "                    advantage_t += discount*(rewards[k] + self.gamma*values[k+1]*(1-int(dones[k])) - values[k] )\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantages[t] = advantage_t\n",
        "\n",
        "            advantages = T.tensor(advantages).to(self.actor.device)\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "\n",
        "            for batch in batches:\n",
        "                states = T.tensor(states[batches], dtype=T.float).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_probs[batches]).to(self.actor.device)\n",
        "                actions = T.tensor(actions[batches]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                predicted_values = self.critic(states)\n",
        "                predicted_values = T.squeeze(predicted_values)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = (new_probs-old_probs).exp()  #new_probs.exp() / old_probs.exp() \n",
        "                actor_loss =  -T.min(\n",
        "                    prob_ratio*advantages[batch], \n",
        "                    T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantages[batch]\n",
        "                ).mean()\n",
        "                \n",
        "                returns = advantages[batch] + values[batch]\n",
        "                critic_loss = ((returns - predicted_values)**2).mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "                \n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "        self.memory.clear_memory()"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykY0scWj4f43"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c_isifJIMRs",
        "outputId": "66a8bd8b-900f-4d5c-9b62-cf4b244f7c24"
      },
      "source": [
        "n_games = 300\n",
        "agent = Agent(n_actions=env.action_space.n, input_dims=env.observation_space.shape, \n",
        "              learn_batch_size=64, actor_learning_rate=0.0003, critic_learning_rate=0.0003, learn_epochs=10)\n",
        "break_to_learn = 20\n",
        "\n",
        "avg_score = 0\n",
        "time_steps = 0\n",
        "learn_steps = 0\n",
        "best_score = env.reward_range[0]\n",
        "score_history = []\n",
        "for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action, prob, value = agent.choose_action(observation)\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        time_steps += 1\n",
        "        score += reward        \n",
        "        agent.store_step(observation, action, prob, value, reward, next_observation, done)\n",
        "        if (time_steps+1) % break_to_learn == 0:\n",
        "            agent.learn()\n",
        "            learn_steps += 1\n",
        "        observation = next_observation        \n",
        "        \n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-10:])\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        agent.save_models('./trained_models')\n",
        "\n",
        "    print('episode', i, 'time_steps', time_steps, 'learning_steps', learn_steps, 'score %.1f' % score, 'avg score %.1f' % avg_score)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0 time_steps 16 learning_steps 0 score 16.0 avg score 16.0\n",
            "episode 1 time_steps 33 learning_steps 1 score 17.0 avg score 16.5\n",
            "episode 2 time_steps 56 learning_steps 2 score 23.0 avg score 18.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode 3 time_steps 103 learning_steps 5 score 47.0 avg score 25.8\n",
            "episode 4 time_steps 146 learning_steps 7 score 43.0 avg score 29.2\n",
            "episode 5 time_steps 224 learning_steps 11 score 78.0 avg score 37.3\n",
            "episode 6 time_steps 242 learning_steps 12 score 18.0 avg score 34.6\n",
            "episode 7 time_steps 296 learning_steps 14 score 54.0 avg score 37.0\n",
            "episode 8 time_steps 317 learning_steps 15 score 21.0 avg score 35.2\n",
            "episode 9 time_steps 330 learning_steps 16 score 13.0 avg score 33.0\n",
            "episode 10 time_steps 349 learning_steps 17 score 19.0 avg score 33.3\n",
            "episode 11 time_steps 374 learning_steps 18 score 25.0 avg score 34.1\n",
            "episode 12 time_steps 388 learning_steps 19 score 14.0 avg score 33.2\n",
            "episode 13 time_steps 473 learning_steps 23 score 85.0 avg score 37.0\n",
            "episode 14 time_steps 504 learning_steps 25 score 31.0 avg score 35.8\n",
            "episode 15 time_steps 589 learning_steps 29 score 85.0 avg score 36.5\n",
            "episode 16 time_steps 658 learning_steps 32 score 69.0 avg score 41.6\n",
            "episode 17 time_steps 692 learning_steps 34 score 34.0 avg score 39.6\n",
            "episode 18 time_steps 722 learning_steps 36 score 30.0 avg score 40.5\n",
            "episode 19 time_steps 751 learning_steps 37 score 29.0 avg score 42.1\n",
            "episode 20 time_steps 766 learning_steps 38 score 15.0 avg score 41.7\n",
            "episode 21 time_steps 867 learning_steps 43 score 101.0 avg score 49.3\n",
            "episode 22 time_steps 902 learning_steps 45 score 35.0 avg score 51.4\n",
            "episode 23 time_steps 943 learning_steps 47 score 41.0 avg score 47.0\n",
            "episode 24 time_steps 968 learning_steps 48 score 25.0 avg score 46.4\n",
            "episode 25 time_steps 989 learning_steps 49 score 21.0 avg score 40.0\n",
            "episode 26 time_steps 1012 learning_steps 50 score 23.0 avg score 35.4\n",
            "episode 27 time_steps 1024 learning_steps 51 score 12.0 avg score 33.2\n",
            "episode 28 time_steps 1042 learning_steps 52 score 18.0 avg score 32.0\n",
            "episode 29 time_steps 1062 learning_steps 53 score 20.0 avg score 31.1\n",
            "episode 30 time_steps 1085 learning_steps 54 score 23.0 avg score 31.9\n",
            "episode 31 time_steps 1102 learning_steps 55 score 17.0 avg score 23.5\n",
            "episode 32 time_steps 1115 learning_steps 55 score 13.0 avg score 21.3\n",
            "episode 33 time_steps 1134 learning_steps 56 score 19.0 avg score 19.1\n",
            "episode 34 time_steps 1213 learning_steps 60 score 79.0 avg score 24.5\n",
            "episode 35 time_steps 1376 learning_steps 68 score 163.0 avg score 38.7\n",
            "episode 36 time_steps 1515 learning_steps 75 score 139.0 avg score 50.3\n",
            "episode 37 time_steps 1553 learning_steps 77 score 38.0 avg score 52.9\n",
            "episode 38 time_steps 1581 learning_steps 79 score 28.0 avg score 53.9\n",
            "episode 39 time_steps 1598 learning_steps 79 score 17.0 avg score 53.6\n",
            "episode 40 time_steps 1636 learning_steps 81 score 38.0 avg score 55.1\n",
            "episode 41 time_steps 1707 learning_steps 85 score 71.0 avg score 60.5\n",
            "episode 42 time_steps 1872 learning_steps 93 score 165.0 avg score 75.7\n",
            "episode 43 time_steps 1909 learning_steps 95 score 37.0 avg score 77.5\n",
            "episode 44 time_steps 1939 learning_steps 97 score 30.0 avg score 72.6\n",
            "episode 45 time_steps 2035 learning_steps 101 score 96.0 avg score 65.9\n",
            "episode 46 time_steps 2144 learning_steps 107 score 109.0 avg score 62.9\n",
            "episode 47 time_steps 2268 learning_steps 113 score 124.0 avg score 71.5\n",
            "episode 48 time_steps 2327 learning_steps 116 score 59.0 avg score 74.6\n",
            "episode 49 time_steps 2425 learning_steps 121 score 98.0 avg score 82.7\n",
            "episode 50 time_steps 2476 learning_steps 123 score 51.0 avg score 84.0\n",
            "episode 51 time_steps 2592 learning_steps 129 score 116.0 avg score 88.5\n",
            "episode 52 time_steps 2619 learning_steps 131 score 27.0 avg score 74.7\n",
            "episode 53 time_steps 2650 learning_steps 132 score 31.0 avg score 74.1\n",
            "episode 54 time_steps 2677 learning_steps 133 score 27.0 avg score 73.8\n",
            "episode 55 time_steps 2801 learning_steps 140 score 124.0 avg score 76.6\n",
            "episode 56 time_steps 2939 learning_steps 147 score 138.0 avg score 79.5\n",
            "episode 57 time_steps 3046 learning_steps 152 score 107.0 avg score 77.8\n",
            "episode 58 time_steps 3190 learning_steps 159 score 144.0 avg score 86.3\n",
            "episode 59 time_steps 3390 learning_steps 169 score 200.0 avg score 96.5\n",
            "episode 60 time_steps 3503 learning_steps 175 score 113.0 avg score 102.7\n",
            "episode 61 time_steps 3583 learning_steps 179 score 80.0 avg score 99.1\n",
            "episode 62 time_steps 3753 learning_steps 187 score 170.0 avg score 113.4\n",
            "episode 63 time_steps 3884 learning_steps 194 score 131.0 avg score 123.4\n",
            "episode 64 time_steps 3978 learning_steps 198 score 94.0 avg score 130.1\n",
            "episode 65 time_steps 3998 learning_steps 199 score 20.0 avg score 119.7\n",
            "episode 66 time_steps 4017 learning_steps 200 score 19.0 avg score 107.8\n",
            "episode 67 time_steps 4078 learning_steps 203 score 61.0 avg score 103.2\n",
            "episode 68 time_steps 4278 learning_steps 213 score 200.0 avg score 108.8\n",
            "episode 69 time_steps 4478 learning_steps 223 score 200.0 avg score 108.8\n",
            "episode 70 time_steps 4674 learning_steps 233 score 196.0 avg score 117.1\n",
            "episode 71 time_steps 4768 learning_steps 238 score 94.0 avg score 118.5\n",
            "episode 72 time_steps 4923 learning_steps 246 score 155.0 avg score 117.0\n",
            "episode 73 time_steps 5021 learning_steps 251 score 98.0 avg score 113.7\n",
            "episode 74 time_steps 5163 learning_steps 258 score 142.0 avg score 118.5\n",
            "episode 75 time_steps 5236 learning_steps 261 score 73.0 avg score 123.8\n",
            "episode 76 time_steps 5436 learning_steps 271 score 200.0 avg score 141.9\n",
            "episode 77 time_steps 5551 learning_steps 277 score 115.0 avg score 147.3\n",
            "episode 78 time_steps 5612 learning_steps 280 score 61.0 avg score 133.4\n",
            "episode 79 time_steps 5699 learning_steps 285 score 87.0 avg score 122.1\n",
            "episode 80 time_steps 5780 learning_steps 289 score 81.0 avg score 110.6\n",
            "episode 81 time_steps 5814 learning_steps 290 score 34.0 avg score 104.6\n",
            "episode 82 time_steps 5866 learning_steps 293 score 52.0 avg score 94.3\n",
            "episode 83 time_steps 5993 learning_steps 299 score 127.0 avg score 97.2\n",
            "episode 84 time_steps 6101 learning_steps 305 score 108.0 avg score 93.8\n",
            "episode 85 time_steps 6170 learning_steps 308 score 69.0 avg score 93.4\n",
            "episode 86 time_steps 6245 learning_steps 312 score 75.0 avg score 80.9\n",
            "episode 87 time_steps 6343 learning_steps 317 score 98.0 avg score 79.2\n",
            "episode 88 time_steps 6425 learning_steps 321 score 82.0 avg score 81.3\n",
            "episode 89 time_steps 6556 learning_steps 327 score 131.0 avg score 85.7\n",
            "episode 90 time_steps 6672 learning_steps 333 score 116.0 avg score 89.2\n",
            "episode 91 time_steps 6799 learning_steps 340 score 127.0 avg score 98.5\n",
            "episode 92 time_steps 6892 learning_steps 344 score 93.0 avg score 102.6\n",
            "episode 93 time_steps 6991 learning_steps 349 score 99.0 avg score 99.8\n",
            "episode 94 time_steps 7117 learning_steps 355 score 126.0 avg score 101.6\n",
            "episode 95 time_steps 7247 learning_steps 362 score 130.0 avg score 107.7\n",
            "episode 96 time_steps 7386 learning_steps 369 score 139.0 avg score 114.1\n",
            "episode 97 time_steps 7513 learning_steps 375 score 127.0 avg score 117.0\n",
            "episode 98 time_steps 7611 learning_steps 380 score 98.0 avg score 118.6\n",
            "episode 99 time_steps 7741 learning_steps 387 score 130.0 avg score 118.5\n",
            "episode 100 time_steps 7846 learning_steps 392 score 105.0 avg score 117.4\n",
            "episode 101 time_steps 7977 learning_steps 398 score 131.0 avg score 117.8\n",
            "episode 102 time_steps 8117 learning_steps 405 score 140.0 avg score 122.5\n",
            "episode 103 time_steps 8250 learning_steps 412 score 133.0 avg score 125.9\n",
            "episode 104 time_steps 8382 learning_steps 419 score 132.0 avg score 126.5\n",
            "episode 105 time_steps 8519 learning_steps 426 score 137.0 avg score 127.2\n",
            "episode 106 time_steps 8659 learning_steps 433 score 140.0 avg score 127.3\n",
            "episode 107 time_steps 8827 learning_steps 441 score 168.0 avg score 131.4\n",
            "episode 108 time_steps 8960 learning_steps 448 score 133.0 avg score 134.9\n",
            "episode 109 time_steps 9099 learning_steps 455 score 139.0 avg score 135.8\n",
            "episode 110 time_steps 9238 learning_steps 461 score 139.0 avg score 139.2\n",
            "episode 111 time_steps 9397 learning_steps 469 score 159.0 avg score 142.0\n",
            "episode 112 time_steps 9574 learning_steps 478 score 177.0 avg score 145.7\n",
            "episode 113 time_steps 9729 learning_steps 486 score 155.0 avg score 147.9\n",
            "episode 114 time_steps 9921 learning_steps 496 score 192.0 avg score 153.9\n",
            "episode 115 time_steps 10089 learning_steps 504 score 168.0 avg score 157.0\n",
            "episode 116 time_steps 10209 learning_steps 510 score 120.0 avg score 155.0\n",
            "episode 117 time_steps 10388 learning_steps 519 score 179.0 avg score 156.1\n",
            "episode 118 time_steps 10588 learning_steps 529 score 200.0 avg score 162.8\n",
            "episode 119 time_steps 10788 learning_steps 539 score 200.0 avg score 168.9\n",
            "episode 120 time_steps 10988 learning_steps 549 score 200.0 avg score 175.0\n",
            "episode 121 time_steps 11172 learning_steps 558 score 184.0 avg score 177.5\n",
            "episode 122 time_steps 11367 learning_steps 568 score 195.0 avg score 179.3\n",
            "episode 123 time_steps 11507 learning_steps 575 score 140.0 avg score 177.8\n",
            "episode 124 time_steps 11563 learning_steps 578 score 56.0 avg score 164.2\n",
            "episode 125 time_steps 11644 learning_steps 582 score 81.0 avg score 155.5\n",
            "episode 126 time_steps 11759 learning_steps 588 score 115.0 avg score 155.0\n",
            "episode 127 time_steps 11917 learning_steps 595 score 158.0 avg score 152.9\n",
            "episode 128 time_steps 12082 learning_steps 604 score 165.0 avg score 149.4\n",
            "episode 129 time_steps 12282 learning_steps 614 score 200.0 avg score 149.4\n",
            "episode 130 time_steps 12482 learning_steps 624 score 200.0 avg score 149.4\n",
            "episode 131 time_steps 12682 learning_steps 634 score 200.0 avg score 151.0\n",
            "episode 132 time_steps 12882 learning_steps 644 score 200.0 avg score 151.5\n",
            "episode 133 time_steps 13082 learning_steps 654 score 200.0 avg score 157.5\n",
            "episode 134 time_steps 13282 learning_steps 664 score 200.0 avg score 171.9\n",
            "episode 135 time_steps 13482 learning_steps 674 score 200.0 avg score 183.8\n",
            "episode 136 time_steps 13682 learning_steps 684 score 200.0 avg score 192.3\n",
            "episode 137 time_steps 13882 learning_steps 694 score 200.0 avg score 196.5\n",
            "episode 138 time_steps 14082 learning_steps 704 score 200.0 avg score 200.0\n",
            "episode 139 time_steps 14282 learning_steps 714 score 200.0 avg score 200.0\n",
            "episode 140 time_steps 14462 learning_steps 723 score 180.0 avg score 198.0\n",
            "episode 141 time_steps 14662 learning_steps 733 score 200.0 avg score 198.0\n",
            "episode 142 time_steps 14799 learning_steps 740 score 137.0 avg score 191.7\n",
            "episode 143 time_steps 14999 learning_steps 750 score 200.0 avg score 191.7\n",
            "episode 144 time_steps 15199 learning_steps 760 score 200.0 avg score 191.7\n",
            "episode 145 time_steps 15399 learning_steps 770 score 200.0 avg score 191.7\n",
            "episode 146 time_steps 15599 learning_steps 780 score 200.0 avg score 191.7\n",
            "episode 147 time_steps 15799 learning_steps 790 score 200.0 avg score 191.7\n",
            "episode 148 time_steps 15999 learning_steps 800 score 200.0 avg score 191.7\n",
            "episode 149 time_steps 16199 learning_steps 810 score 200.0 avg score 191.7\n",
            "episode 150 time_steps 16399 learning_steps 820 score 200.0 avg score 193.7\n",
            "episode 151 time_steps 16599 learning_steps 830 score 200.0 avg score 193.7\n",
            "episode 152 time_steps 16799 learning_steps 840 score 200.0 avg score 200.0\n",
            "episode 153 time_steps 16999 learning_steps 850 score 200.0 avg score 200.0\n",
            "episode 154 time_steps 17199 learning_steps 860 score 200.0 avg score 200.0\n",
            "episode 155 time_steps 17399 learning_steps 870 score 200.0 avg score 200.0\n",
            "episode 156 time_steps 17599 learning_steps 880 score 200.0 avg score 200.0\n",
            "episode 157 time_steps 17799 learning_steps 890 score 200.0 avg score 200.0\n",
            "episode 158 time_steps 17999 learning_steps 900 score 200.0 avg score 200.0\n",
            "episode 159 time_steps 18199 learning_steps 910 score 200.0 avg score 200.0\n",
            "episode 160 time_steps 18399 learning_steps 920 score 200.0 avg score 200.0\n",
            "episode 161 time_steps 18599 learning_steps 930 score 200.0 avg score 200.0\n",
            "episode 162 time_steps 18799 learning_steps 940 score 200.0 avg score 200.0\n",
            "episode 163 time_steps 18999 learning_steps 950 score 200.0 avg score 200.0\n",
            "episode 164 time_steps 19199 learning_steps 960 score 200.0 avg score 200.0\n",
            "episode 165 time_steps 19399 learning_steps 970 score 200.0 avg score 200.0\n",
            "episode 166 time_steps 19599 learning_steps 980 score 200.0 avg score 200.0\n",
            "episode 167 time_steps 19799 learning_steps 990 score 200.0 avg score 200.0\n",
            "episode 168 time_steps 19999 learning_steps 1000 score 200.0 avg score 200.0\n",
            "episode 169 time_steps 20199 learning_steps 1010 score 200.0 avg score 200.0\n",
            "episode 170 time_steps 20399 learning_steps 1020 score 200.0 avg score 200.0\n",
            "episode 171 time_steps 20599 learning_steps 1030 score 200.0 avg score 200.0\n",
            "episode 172 time_steps 20799 learning_steps 1040 score 200.0 avg score 200.0\n",
            "episode 173 time_steps 20999 learning_steps 1050 score 200.0 avg score 200.0\n",
            "episode 174 time_steps 21199 learning_steps 1060 score 200.0 avg score 200.0\n",
            "episode 175 time_steps 21399 learning_steps 1070 score 200.0 avg score 200.0\n",
            "episode 176 time_steps 21599 learning_steps 1080 score 200.0 avg score 200.0\n",
            "episode 177 time_steps 21799 learning_steps 1090 score 200.0 avg score 200.0\n",
            "episode 178 time_steps 21999 learning_steps 1100 score 200.0 avg score 200.0\n",
            "episode 179 time_steps 22199 learning_steps 1110 score 200.0 avg score 200.0\n",
            "episode 180 time_steps 22399 learning_steps 1120 score 200.0 avg score 200.0\n",
            "episode 181 time_steps 22599 learning_steps 1130 score 200.0 avg score 200.0\n",
            "episode 182 time_steps 22799 learning_steps 1140 score 200.0 avg score 200.0\n",
            "episode 183 time_steps 22999 learning_steps 1150 score 200.0 avg score 200.0\n",
            "episode 184 time_steps 23199 learning_steps 1160 score 200.0 avg score 200.0\n",
            "episode 185 time_steps 23399 learning_steps 1170 score 200.0 avg score 200.0\n",
            "episode 186 time_steps 23599 learning_steps 1180 score 200.0 avg score 200.0\n",
            "episode 187 time_steps 23799 learning_steps 1190 score 200.0 avg score 200.0\n",
            "episode 188 time_steps 23999 learning_steps 1200 score 200.0 avg score 200.0\n",
            "episode 189 time_steps 24199 learning_steps 1210 score 200.0 avg score 200.0\n",
            "episode 190 time_steps 24399 learning_steps 1220 score 200.0 avg score 200.0\n",
            "episode 191 time_steps 24599 learning_steps 1230 score 200.0 avg score 200.0\n",
            "episode 192 time_steps 24799 learning_steps 1240 score 200.0 avg score 200.0\n",
            "episode 193 time_steps 24999 learning_steps 1250 score 200.0 avg score 200.0\n",
            "episode 194 time_steps 25199 learning_steps 1260 score 200.0 avg score 200.0\n",
            "episode 195 time_steps 25399 learning_steps 1270 score 200.0 avg score 200.0\n",
            "episode 196 time_steps 25599 learning_steps 1280 score 200.0 avg score 200.0\n",
            "episode 197 time_steps 25799 learning_steps 1290 score 200.0 avg score 200.0\n",
            "episode 198 time_steps 25999 learning_steps 1300 score 200.0 avg score 200.0\n",
            "episode 199 time_steps 26199 learning_steps 1310 score 200.0 avg score 200.0\n",
            "episode 200 time_steps 26399 learning_steps 1320 score 200.0 avg score 200.0\n",
            "episode 201 time_steps 26599 learning_steps 1330 score 200.0 avg score 200.0\n",
            "episode 202 time_steps 26799 learning_steps 1340 score 200.0 avg score 200.0\n",
            "episode 203 time_steps 26999 learning_steps 1350 score 200.0 avg score 200.0\n",
            "episode 204 time_steps 27199 learning_steps 1360 score 200.0 avg score 200.0\n",
            "episode 205 time_steps 27399 learning_steps 1370 score 200.0 avg score 200.0\n",
            "episode 206 time_steps 27599 learning_steps 1380 score 200.0 avg score 200.0\n",
            "episode 207 time_steps 27799 learning_steps 1390 score 200.0 avg score 200.0\n",
            "episode 208 time_steps 27999 learning_steps 1400 score 200.0 avg score 200.0\n",
            "episode 209 time_steps 28199 learning_steps 1410 score 200.0 avg score 200.0\n",
            "episode 210 time_steps 28399 learning_steps 1420 score 200.0 avg score 200.0\n",
            "episode 211 time_steps 28599 learning_steps 1430 score 200.0 avg score 200.0\n",
            "episode 212 time_steps 28799 learning_steps 1440 score 200.0 avg score 200.0\n",
            "episode 213 time_steps 28999 learning_steps 1450 score 200.0 avg score 200.0\n",
            "episode 214 time_steps 29199 learning_steps 1460 score 200.0 avg score 200.0\n",
            "episode 215 time_steps 29399 learning_steps 1470 score 200.0 avg score 200.0\n",
            "episode 216 time_steps 29599 learning_steps 1480 score 200.0 avg score 200.0\n",
            "episode 217 time_steps 29799 learning_steps 1490 score 200.0 avg score 200.0\n",
            "episode 218 time_steps 29999 learning_steps 1500 score 200.0 avg score 200.0\n",
            "episode 219 time_steps 30199 learning_steps 1510 score 200.0 avg score 200.0\n",
            "episode 220 time_steps 30399 learning_steps 1520 score 200.0 avg score 200.0\n",
            "episode 221 time_steps 30599 learning_steps 1530 score 200.0 avg score 200.0\n",
            "episode 222 time_steps 30799 learning_steps 1540 score 200.0 avg score 200.0\n",
            "episode 223 time_steps 30999 learning_steps 1550 score 200.0 avg score 200.0\n",
            "episode 224 time_steps 31199 learning_steps 1560 score 200.0 avg score 200.0\n",
            "episode 225 time_steps 31399 learning_steps 1570 score 200.0 avg score 200.0\n",
            "episode 226 time_steps 31599 learning_steps 1580 score 200.0 avg score 200.0\n",
            "episode 227 time_steps 31799 learning_steps 1590 score 200.0 avg score 200.0\n",
            "episode 228 time_steps 31999 learning_steps 1600 score 200.0 avg score 200.0\n",
            "episode 229 time_steps 32199 learning_steps 1610 score 200.0 avg score 200.0\n",
            "episode 230 time_steps 32399 learning_steps 1620 score 200.0 avg score 200.0\n",
            "episode 231 time_steps 32599 learning_steps 1630 score 200.0 avg score 200.0\n",
            "episode 232 time_steps 32799 learning_steps 1640 score 200.0 avg score 200.0\n",
            "episode 233 time_steps 32999 learning_steps 1650 score 200.0 avg score 200.0\n",
            "episode 234 time_steps 33199 learning_steps 1660 score 200.0 avg score 200.0\n",
            "episode 235 time_steps 33399 learning_steps 1670 score 200.0 avg score 200.0\n",
            "episode 236 time_steps 33599 learning_steps 1680 score 200.0 avg score 200.0\n",
            "episode 237 time_steps 33799 learning_steps 1690 score 200.0 avg score 200.0\n",
            "episode 238 time_steps 33999 learning_steps 1700 score 200.0 avg score 200.0\n",
            "episode 239 time_steps 34199 learning_steps 1710 score 200.0 avg score 200.0\n",
            "episode 240 time_steps 34399 learning_steps 1720 score 200.0 avg score 200.0\n",
            "episode 241 time_steps 34599 learning_steps 1730 score 200.0 avg score 200.0\n",
            "episode 242 time_steps 34799 learning_steps 1740 score 200.0 avg score 200.0\n",
            "episode 243 time_steps 34999 learning_steps 1750 score 200.0 avg score 200.0\n",
            "episode 244 time_steps 35199 learning_steps 1760 score 200.0 avg score 200.0\n",
            "episode 245 time_steps 35399 learning_steps 1770 score 200.0 avg score 200.0\n",
            "episode 246 time_steps 35599 learning_steps 1780 score 200.0 avg score 200.0\n",
            "episode 247 time_steps 35799 learning_steps 1790 score 200.0 avg score 200.0\n",
            "episode 248 time_steps 35999 learning_steps 1800 score 200.0 avg score 200.0\n",
            "episode 249 time_steps 36199 learning_steps 1810 score 200.0 avg score 200.0\n",
            "episode 250 time_steps 36399 learning_steps 1820 score 200.0 avg score 200.0\n",
            "episode 251 time_steps 36599 learning_steps 1830 score 200.0 avg score 200.0\n",
            "episode 252 time_steps 36799 learning_steps 1840 score 200.0 avg score 200.0\n",
            "episode 253 time_steps 36999 learning_steps 1850 score 200.0 avg score 200.0\n",
            "episode 254 time_steps 37137 learning_steps 1856 score 138.0 avg score 193.8\n",
            "episode 255 time_steps 37337 learning_steps 1866 score 200.0 avg score 193.8\n",
            "episode 256 time_steps 37537 learning_steps 1876 score 200.0 avg score 193.8\n",
            "episode 257 time_steps 37737 learning_steps 1886 score 200.0 avg score 193.8\n",
            "episode 258 time_steps 37937 learning_steps 1896 score 200.0 avg score 193.8\n",
            "episode 259 time_steps 38137 learning_steps 1906 score 200.0 avg score 193.8\n",
            "episode 260 time_steps 38337 learning_steps 1916 score 200.0 avg score 193.8\n",
            "episode 261 time_steps 38537 learning_steps 1926 score 200.0 avg score 193.8\n",
            "episode 262 time_steps 38737 learning_steps 1936 score 200.0 avg score 193.8\n",
            "episode 263 time_steps 38937 learning_steps 1946 score 200.0 avg score 193.8\n",
            "episode 264 time_steps 39137 learning_steps 1956 score 200.0 avg score 200.0\n",
            "episode 265 time_steps 39337 learning_steps 1966 score 200.0 avg score 200.0\n",
            "episode 266 time_steps 39537 learning_steps 1976 score 200.0 avg score 200.0\n",
            "episode 267 time_steps 39737 learning_steps 1986 score 200.0 avg score 200.0\n",
            "episode 268 time_steps 39937 learning_steps 1996 score 200.0 avg score 200.0\n",
            "episode 269 time_steps 40137 learning_steps 2006 score 200.0 avg score 200.0\n",
            "episode 270 time_steps 40337 learning_steps 2016 score 200.0 avg score 200.0\n",
            "episode 271 time_steps 40537 learning_steps 2026 score 200.0 avg score 200.0\n",
            "episode 272 time_steps 40737 learning_steps 2036 score 200.0 avg score 200.0\n",
            "episode 273 time_steps 40883 learning_steps 2044 score 146.0 avg score 194.6\n",
            "episode 274 time_steps 41067 learning_steps 2053 score 184.0 avg score 193.0\n",
            "episode 275 time_steps 41267 learning_steps 2063 score 200.0 avg score 193.0\n",
            "episode 276 time_steps 41467 learning_steps 2073 score 200.0 avg score 193.0\n",
            "episode 277 time_steps 41667 learning_steps 2083 score 200.0 avg score 193.0\n",
            "episode 278 time_steps 41867 learning_steps 2093 score 200.0 avg score 193.0\n",
            "episode 279 time_steps 41994 learning_steps 2099 score 127.0 avg score 185.7\n",
            "episode 280 time_steps 42194 learning_steps 2109 score 200.0 avg score 185.7\n",
            "episode 281 time_steps 42361 learning_steps 2118 score 167.0 avg score 182.4\n",
            "episode 282 time_steps 42487 learning_steps 2124 score 126.0 avg score 175.0\n",
            "episode 283 time_steps 42662 learning_steps 2133 score 175.0 avg score 177.9\n",
            "episode 284 time_steps 42807 learning_steps 2140 score 145.0 avg score 174.0\n",
            "episode 285 time_steps 43007 learning_steps 2150 score 200.0 avg score 174.0\n",
            "episode 286 time_steps 43207 learning_steps 2160 score 200.0 avg score 174.0\n",
            "episode 287 time_steps 43256 learning_steps 2162 score 49.0 avg score 158.9\n",
            "episode 288 time_steps 43346 learning_steps 2167 score 90.0 avg score 147.9\n",
            "episode 289 time_steps 43396 learning_steps 2169 score 50.0 avg score 140.2\n",
            "episode 290 time_steps 43535 learning_steps 2176 score 139.0 avg score 134.1\n",
            "episode 291 time_steps 43681 learning_steps 2184 score 146.0 avg score 132.0\n",
            "episode 292 time_steps 43825 learning_steps 2191 score 144.0 avg score 133.8\n",
            "episode 293 time_steps 44025 learning_steps 2201 score 200.0 avg score 136.3\n",
            "episode 294 time_steps 44190 learning_steps 2209 score 165.0 avg score 138.3\n",
            "episode 295 time_steps 44358 learning_steps 2217 score 168.0 avg score 135.1\n",
            "episode 296 time_steps 44558 learning_steps 2227 score 200.0 avg score 135.1\n",
            "episode 297 time_steps 44628 learning_steps 2231 score 70.0 avg score 137.2\n",
            "episode 298 time_steps 44752 learning_steps 2237 score 124.0 avg score 140.6\n",
            "episode 299 time_steps 44874 learning_steps 2243 score 122.0 avg score 147.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5KR-RkS4i93"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUTWweT03VRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8db853-4fe7-4705-84f4-126212e6328c"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action, _, _ = agent.choose_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 70.0\n",
            "Episode: 1 score: 124.0\n",
            "Episode: 2 score: 75.0\n",
            "Episode: 3 score: 128.0\n",
            "Episode: 4 score: 122.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hoefHWwP3jE",
        "outputId": "12adfebc-3e9a-4c63-8e03-3463e36f90bc"
      },
      "source": [
        "agent = Agent(n_actions=env.action_space.n, input_dims=env.observation_space.shape, \n",
        "              learn_batch_size=64, actor_learning_rate=0.0003, critic_learning_rate=0.0003, learn_epochs=10)\n",
        "\n",
        "state = env.reset()\n",
        "action, prob, _ = agent.choose_action(state)\n",
        "print(state, action)\n",
        "print()\n",
        "state, reward, done, info = env.step(0)\n",
        "action, prob, _ = agent.choose_action(state)\n",
        "print(state, action)\n",
        "print()\n",
        "state, reward, done, info = env.step(0)\n",
        "action, prob, _ = agent.choose_action(state)\n",
        "print(state, action)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.01823638 -0.00567572 -0.03260454  0.01748912] 0\n",
            "\n",
            "[ 0.01812287 -0.20031529 -0.03225476  0.29970926] 1\n",
            "\n",
            "[ 0.01411656 -0.39496299 -0.02626057  0.58204769] 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuCo8r734nQb"
      },
      "source": [
        "# PPO tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNaQ7TEY4nQd"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmrkGO-s4nQe"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOwvvQiI4nQg"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWe_oqBC4nQh"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t66JBkJA4nQi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmphIwVZ4nQi"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFNi_q964nQj"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}