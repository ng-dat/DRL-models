{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Simple DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8e3da308",
        "70e322b3",
        "1cc0a6f0",
        "99c18964",
        "jrl2q5tlkUUh"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nguyencongdat1997/RL.TryOut/blob/developments-ppo/Simple_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc011312"
      },
      "source": [
        "# Installization & Import"
      ],
      "id": "cc011312"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "b81d7a86",
        "outputId": "ee1e74e0-1936-48ca-a381-88f080fe83d3"
      },
      "source": [
        "!pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]"
      ],
      "id": "b81d7a86",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 keras-rl2-1.0.5 numpy-1.18.5 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d4450bd"
      },
      "source": [
        "import gym \n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "id": "3d4450bd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e866cf0b"
      },
      "source": [
        "# Environment"
      ],
      "id": "e866cf0b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13bdfec0"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "observations = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "action_space = [x for x in range(actions)]"
      ],
      "id": "13bdfec0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06c25dea",
        "outputId": "c09540ac-39ca-4b3c-920e-889d35e4fdb8"
      },
      "source": [
        "print(actions)\n",
        "sample_action = env.action_space.sample()\n",
        "print(sample_action)\n",
        "print(observations)\n",
        "state = env.reset()\n",
        "print(state)\n",
        "state, reward, done, info = env.step(sample_action)\n",
        "print(state, reward, done, info)"
      ],
      "id": "06c25dea",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "4\n",
            "[0.02315373 0.04200671 0.00834748 0.013585  ]\n",
            "[ 0.02399386  0.23700796  0.00861918 -0.27645256] 1.0 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec66d65b",
        "outputId": "7928825b-0462-4f00-c875-a0d60701505b"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = random.choice(action_space)\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "env.close()"
      ],
      "id": "ec66d65b",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:1 Score:25.0\n",
            "Episode:2 Score:23.0\n",
            "Episode:3 Score:34.0\n",
            "Episode:4 Score:11.0\n",
            "Episode:5 Score:13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e3da308"
      },
      "source": [
        "# KerasRL's DQN"
      ],
      "id": "8e3da308"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70e322b3"
      },
      "source": [
        "## Import"
      ],
      "id": "70e322b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c79a434d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "fb59e649-b4c3-485a-ce28-ffd181b9d763"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy"
      ],
      "id": "c79a434d",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9a52de42a28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearAnnealedPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsGreedyQPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBoltzmannQPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cc0a6f0"
      },
      "source": [
        "## Model"
      ],
      "id": "1cc0a6f0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efdd7f71"
      },
      "source": [
        "def build_model(observations, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "    model.add(Dense(24, activation='tanh'))\n",
        "    model.add(Dense(48, activation='tanh'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "id": "efdd7f71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f64bd546"
      },
      "source": [
        "model = build_model(observations, actions)"
      ],
      "id": "f64bd546",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f81613b7",
        "outputId": "38ef9b38-2ab3-4a97-e72c-ef2c032690fe"
      },
      "source": [
        "model.summary()"
      ],
      "id": "f81613b7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                120       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 48)                1200      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 98        \n",
            "=================================================================\n",
            "Total params: 1,418\n",
            "Trainable params: 1,418\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac65c994"
      },
      "source": [
        "## DQN"
      ],
      "id": "ac65c994"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f87bfcd4"
      },
      "source": [
        "def build_agent(model, actions):\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    policy = BoltzmannQPolicy()\n",
        "    dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=2000,\n",
        "                   target_model_update=1e-2, policy=policy)\n",
        "    dqn.compile(Adam(lr=0.01, decay=0.01), metrics=['mse'])\n",
        "    return dqn"
      ],
      "id": "f87bfcd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "666c9042"
      },
      "source": [
        "dqn = build_agent(model, actions)"
      ],
      "id": "666c9042",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "35a41482",
        "outputId": "f23aa492-3b7f-4418-9865-124dcb5bcf1e"
      },
      "source": [
        "dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"
      ],
      "id": "35a41482",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n",
            "done, took 76.169 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1b85b1d8388>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cac100f0"
      },
      "source": [
        "dqn.save_weights('./trained_models/CartPole/KeraRL/model_10000')"
      ],
      "id": "cac100f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6a199df"
      },
      "source": [
        "## Test"
      ],
      "id": "d6a199df"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b57cbef6"
      },
      "source": [
        "dqn.load_weights('./trained_models/CartPole/KeraRL/model_10000')"
      ],
      "id": "b57cbef6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c125590",
        "outputId": "47a927f2-8f61-42a0-e0f4-6d1628f093a8"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "id": "1c125590",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d005524",
        "outputId": "9c607654-1211-413d-c177-302fa87f6279"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action = dqn.forward(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "id": "5d005524",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 200.0\n",
            "Episode: 1 score: 200.0\n",
            "Episode: 2 score: 200.0\n",
            "Episode: 3 score: 200.0\n",
            "Episode: 4 score: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c18964"
      },
      "source": [
        "# Stable baseline"
      ],
      "id": "99c18964"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0410385b"
      },
      "source": [
        "## Import"
      ],
      "id": "0410385b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3dd36ce"
      },
      "source": [
        "from stable_baselines3.common.cmd_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3 import PPO, DQN, A2C\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import numpy as np \n",
        "import os## Import"
      ],
      "id": "a3dd36ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae4e04aa"
      },
      "source": [
        "## Callback"
      ],
      "id": "ae4e04aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6b3ce28"
      },
      "source": [
        "class SavingBestTrainingRewardCallback(BaseCallback):\n",
        "    def __init__(self, check_freq:int, save_path: str, verbose=1):\n",
        "        super(SavingBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.save_path = save_path\n",
        "    def _init_callback(self):\n",
        "        if self.save_path:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = os.path.join(self.save_path, 'model_{}'.format(self.n_calls))\n",
        "            self.model.save(model_path)\n",
        "        return True"
      ],
      "id": "e6b3ce28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a582de2e"
      },
      "source": [
        "CHECKPOINT_DIR = './trained_models/CartPole/StableBaselines/'\n",
        "LOG_DIR = './logs/CartPole/StableBaselines/'\n",
        "callback = SavingBestTrainingRewardCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
      ],
      "id": "a582de2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "716a6148"
      },
      "source": [
        "## Train"
      ],
      "id": "716a6148"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "848ce634"
      },
      "source": [
        "agent = A2C('MlpPolicy', env, verbose=0, tensorboard_log=LOG_DIR)\n",
        "# agent = DQN('MlpPolicy', env, verbose=0, tensorboard_log=LOG_DIR)\n",
        "#agent = ACER('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR)\n",
        "#agent = PPO2('CnnPolicy', env, minibaches=2, verbose=1, tensorboard_log=LOG_DIR)\n",
        "#agent = DQN('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR)"
      ],
      "id": "848ce634",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66e2d10f"
      },
      "source": [
        "# trained_agent = A2C.load('./train/model_10000', env=env, tensorboard_log=LOG_DIR)"
      ],
      "id": "66e2d10f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "411d4c48",
        "outputId": "f5942313-0c22-4a7c-9231-19f9d92c5874"
      },
      "source": [
        "agent.learn(total_timesteps= 20000, callback= callback)"
      ],
      "id": "411d4c48",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x1bb5fae4908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8119e0ef"
      },
      "source": [
        "## Test"
      ],
      "id": "8119e0ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ec0994"
      },
      "source": [
        "agent = A2C.load(CHECKPOINT_DIR + '/model_20000', env=env)"
      ],
      "id": "21ec0994",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db76df9c",
        "outputId": "d20ec380-be0b-4910-814a-ab0b4020db48"
      },
      "source": [
        "evaluate_policy(agent, env, n_eval_episodes=10, render=True)"
      ],
      "id": "db76df9c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1bfa415",
        "outputId": "4e44379e-58e8-42dd-9198-d596a01d9a53"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action, states = agent.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "id": "e1bfa415",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 197.0\n",
            "Episode: 1 score: 37.0\n",
            "Episode: 2 score: 191.0\n",
            "Episode: 3 score: 118.0\n",
            "Episode: 4 score: 199.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792526cf"
      },
      "source": [
        "# From scratch - Double Dueling Deep Q - Keras"
      ],
      "id": "792526cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM8ybh7UQ5kI"
      },
      "source": [
        "## Replay Buffer"
      ],
      "id": "CM8ybh7UQ5kI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfRYechIQ7zO"
      },
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, max_size, input_shape):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "    \n",
        "    self.states = np.zeros((self.mem_size, *input_shape), dtype=np.float64)\n",
        "    self.next_states = np.zeros((self.mem_size, *input_shape), dtype=np.float64)\n",
        "    self.rewards = np.zeros(self.mem_size, dtype=np.float64)\n",
        "    self.actions = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.done = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "  def store_step(self, state, action, reward, next_state, done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.states[index] = state\n",
        "    self.next_states[index] = next_state\n",
        "    self.actions[index] = action\n",
        "    self.rewards[index] = reward\n",
        "    self.done[index] = done\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_counter, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "    states = self.states[batch]\n",
        "    next_states = self.next_states[batch]\n",
        "    rewards = self.rewards[batch]\n",
        "    actions = self.actions[batch]\n",
        "    done = self.done[batch]\n",
        "\n",
        "    return states, actions, rewards, next_states, done\n",
        "    "
      ],
      "id": "kfRYechIQ7zO",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xn9jbTsPjI2"
      },
      "source": [
        "## Q Network"
      ],
      "id": "6Xn9jbTsPjI2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O09zF-WHPhFv"
      },
      "source": [
        "class DuelingDeepQNetwork(keras.Model):\n",
        "  def __init__(self, n_actions):\n",
        "    super(DuelingDeepQNetwork, self).__init__()\n",
        "\n",
        "    fc1_dims = 128\n",
        "    fc2_dims = 128\n",
        "    self.dense1 = keras.layers.Dense(fc1_dims, activation='relu')\n",
        "    self.dense2 = keras.layers.Dense(fc2_dims, activation='relu')\n",
        "    self.V = keras.layers.Dense(1, activation=None)\n",
        "    self.A = keras.layers.Dense(n_actions, activation=None)\n",
        "\n",
        "  def call(self, state):\n",
        "    x = self.dense1(state)\n",
        "    x = self.dense2(x)\n",
        "    V = self.V(x)\n",
        "    A = self.A(x)\n",
        "\n",
        "    Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n",
        "    return Q\n",
        "\n",
        "  def advantage(self, state):\n",
        "    x = self.dense1(state)\n",
        "    x = self.dense2(x)    \n",
        "    A = self.A(x)\n",
        "    return A  \n"
      ],
      "id": "O09zF-WHPhFv",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO4dmDllS5ng"
      },
      "source": [
        "## Agent"
      ],
      "id": "EO4dmDllS5ng"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub0jKpvgS7U2"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, lr, gamma, n_actions, epsilon, batch_size, input_dims, epsilon_dec=1e-3, epsilon_end=0.01, mem_size=1000000, replace=100):\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.gamma =gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_dec = epsilon_dec\n",
        "    self.epsilon_end = epsilon_end\n",
        "    self.replace = replace\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.learned_step_counter = 0\n",
        "    self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "    self.q_active =  DuelingDeepQNetwork(n_actions)\n",
        "    self.q_frozen =  DuelingDeepQNetwork(n_actions)\n",
        "\n",
        "    self.q_active.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "    self.q_frozen.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "  def store_step(self, state, action, reward, next_state, done):\n",
        "    self.memory.store_step(state, action, reward, next_state, done)\n",
        "  \n",
        "  def choose_action(self, observation):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action_space)\n",
        "    else:\n",
        "      state = np.array([observation])\n",
        "      actions = self.q_active.advantage(state)\n",
        "      action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "    \n",
        "    if self.learned_step_counter % self.replace == 0:\n",
        "      self.q_frozen.set_weights(self.q_active.get_weights())\n",
        "\n",
        "    # get data\n",
        "    states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "    q_pred = self.q_active(states)\n",
        "    q_next = self.q_frozen(next_states)\n",
        "    q_target = q_pred.numpy()\n",
        "    max_next_actions = tf.math.argmax(self.q_active(next_states), axis=1)\n",
        "    for i, terminated in enumerate(dones):\n",
        "      q_target[i, actions[i]] = rewards[i] + self.gamma*q_next[i, max_next_actions[i]]*(1-int(dones[i]))\n",
        "\n",
        "    # train\n",
        "    self.q_active.train_on_batch(states, q_target)\n",
        "\n",
        "    self.epsilon = max(self.epsilon - self.epsilon_dec, self.epsilon_end)\n",
        "    self.learned_step_counter += 1\n",
        "\n",
        "  def train(self, env, n_games):\n",
        "    scores = []\n",
        "    eps_history = []\n",
        "    steps = 0\n",
        "    for i in range(n_games):\n",
        "      done = False\n",
        "      score = 0\n",
        "      observation = env.reset()\n",
        "      while not done:\n",
        "        steps += 1\n",
        "        action = self.choose_action(observation)\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        self.store_step(observation, action, reward, next_observation, done)\n",
        "        observation = next_observation\n",
        "        self.learn()\n",
        "      eps_history.append(self.epsilon)\n",
        "      scores.append(score)\n",
        "      avg_score = np.mean(scores[-10:])\n",
        "      print('Episode', i, '- trained steps', steps, '- score %.1f'%score, '- avg_score %.1f ' % avg_score)\n",
        "\n",
        "  def save_model(self, train_dir):\n",
        "    file_name = train_dir + '/d3qn_' + str(self.learned_step_counter) + '/model'\n",
        "    #self.q_active.save_weights(file_name)\n",
        "    self.q_active.save_weights(file_name, save_format='tf')\n",
        "\n",
        "  def load_model(self, train_dir, learned_steps = 100):\n",
        "    file_name = train_dir + '/d3qn_' + str(learned_steps) + '/model' \n",
        "    self.q_active.load_weights(file_name)\n",
        "    self.q_frozen.set_weights(self.q_active.get_weights())\n"
      ],
      "id": "ub0jKpvgS7U2",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcUQlfdjXUIG"
      },
      "source": [
        "## Train"
      ],
      "id": "CcUQlfdjXUIG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxkII7xYXUfk"
      },
      "source": [
        "d3qn = Agent(lr=0.005, gamma=0.99, n_actions=env.action_space.n, epsilon=1.0, batch_size=64, input_dims=env.observation_space.shape)"
      ],
      "id": "OxkII7xYXUfk",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7NUIsVPCZ3nW",
        "outputId": "4c02f81c-7848-45d5-ad0c-d3a3408e5050"
      },
      "source": [
        "n_games = 100\n",
        "d3qn.train(env, n_games)"
      ],
      "id": "7NUIsVPCZ3nW",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0 - trained steps 18 - score 18.0 - avg_score 18.0 \n",
            "Episode 1 - trained steps 38 - score 20.0 - avg_score 19.0 \n",
            "Episode 2 - trained steps 75 - score 37.0 - avg_score 25.0 \n",
            "Episode 3 - trained steps 91 - score 16.0 - avg_score 22.8 \n",
            "Episode 4 - trained steps 103 - score 12.0 - avg_score 20.6 \n",
            "Episode 5 - trained steps 117 - score 14.0 - avg_score 19.5 \n",
            "Episode 6 - trained steps 134 - score 17.0 - avg_score 19.1 \n",
            "Episode 7 - trained steps 196 - score 62.0 - avg_score 24.5 \n",
            "Episode 8 - trained steps 211 - score 15.0 - avg_score 23.4 \n",
            "Episode 9 - trained steps 225 - score 14.0 - avg_score 22.5 \n",
            "Episode 10 - trained steps 248 - score 23.0 - avg_score 23.0 \n",
            "Episode 11 - trained steps 310 - score 62.0 - avg_score 27.2 \n",
            "Episode 12 - trained steps 322 - score 12.0 - avg_score 24.7 \n",
            "Episode 13 - trained steps 392 - score 70.0 - avg_score 30.1 \n",
            "Episode 14 - trained steps 447 - score 55.0 - avg_score 34.4 \n",
            "Episode 15 - trained steps 460 - score 13.0 - avg_score 34.3 \n",
            "Episode 16 - trained steps 479 - score 19.0 - avg_score 34.5 \n",
            "Episode 17 - trained steps 497 - score 18.0 - avg_score 30.1 \n",
            "Episode 18 - trained steps 511 - score 14.0 - avg_score 30.0 \n",
            "Episode 19 - trained steps 545 - score 34.0 - avg_score 32.0 \n",
            "Episode 20 - trained steps 573 - score 28.0 - avg_score 32.5 \n",
            "Episode 21 - trained steps 587 - score 14.0 - avg_score 27.7 \n",
            "Episode 22 - trained steps 602 - score 15.0 - avg_score 28.0 \n",
            "Episode 23 - trained steps 643 - score 41.0 - avg_score 25.1 \n",
            "Episode 24 - trained steps 784 - score 141.0 - avg_score 33.7 \n",
            "Episode 25 - trained steps 859 - score 75.0 - avg_score 39.9 \n",
            "Episode 26 - trained steps 903 - score 44.0 - avg_score 42.4 \n",
            "Episode 27 - trained steps 1031 - score 128.0 - avg_score 53.4 \n",
            "Episode 28 - trained steps 1096 - score 65.0 - avg_score 58.5 \n",
            "Episode 29 - trained steps 1237 - score 141.0 - avg_score 69.2 \n",
            "Episode 30 - trained steps 1354 - score 117.0 - avg_score 78.1 \n",
            "Episode 31 - trained steps 1538 - score 184.0 - avg_score 95.1 \n",
            "Episode 32 - trained steps 1681 - score 143.0 - avg_score 107.9 \n",
            "Episode 33 - trained steps 1812 - score 131.0 - avg_score 116.9 \n",
            "Episode 34 - trained steps 2012 - score 200.0 - avg_score 122.8 \n",
            "Episode 35 - trained steps 2127 - score 115.0 - avg_score 126.8 \n",
            "Episode 36 - trained steps 2249 - score 122.0 - avg_score 134.6 \n",
            "Episode 37 - trained steps 2288 - score 39.0 - avg_score 125.7 \n",
            "Episode 38 - trained steps 2488 - score 200.0 - avg_score 139.2 \n",
            "Episode 39 - trained steps 2684 - score 196.0 - avg_score 144.7 \n",
            "Episode 40 - trained steps 2818 - score 134.0 - avg_score 146.4 \n",
            "Episode 41 - trained steps 3018 - score 200.0 - avg_score 148.0 \n",
            "Episode 42 - trained steps 3218 - score 200.0 - avg_score 153.7 \n",
            "Episode 43 - trained steps 3338 - score 120.0 - avg_score 152.6 \n",
            "Episode 44 - trained steps 3458 - score 120.0 - avg_score 144.6 \n",
            "Episode 45 - trained steps 3558 - score 100.0 - avg_score 143.1 \n",
            "Episode 46 - trained steps 3669 - score 111.0 - avg_score 142.0 \n",
            "Episode 47 - trained steps 3789 - score 120.0 - avg_score 150.1 \n",
            "Episode 48 - trained steps 3921 - score 132.0 - avg_score 143.3 \n",
            "Episode 49 - trained steps 3956 - score 35.0 - avg_score 127.2 \n",
            "Episode 50 - trained steps 4156 - score 200.0 - avg_score 133.8 \n",
            "Episode 51 - trained steps 4237 - score 81.0 - avg_score 121.9 \n",
            "Episode 52 - trained steps 4437 - score 200.0 - avg_score 121.9 \n",
            "Episode 53 - trained steps 4637 - score 200.0 - avg_score 129.9 \n",
            "Episode 54 - trained steps 4833 - score 196.0 - avg_score 137.5 \n",
            "Episode 55 - trained steps 5006 - score 173.0 - avg_score 144.8 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-48c3963eadd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md3qn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-0f0ea301e6b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, n_games)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0meps_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-0f0ea301e6b4>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmax_next_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_next_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1024\u001b[0m       skip_on_eager=False) as name:\n\u001b[1;32m   1025\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                                   stack(strides))\n\u001b[1;32m   1028\u001b[0m       if (packed_begin.dtype == dtypes.int64 or\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1534\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1470\u001b[0m           elems_as_tensors.append(\n\u001b[1;32m   1471\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0;32m-> 1472\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6395\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6396\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 6397\u001b[0;31m         _ctx, \"Pack\", name, values, \"axis\", axis)\n\u001b[0m\u001b[1;32m   6398\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6399\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lio08_Iqg5w",
        "outputId": "c6c452fe-77da-44d3-9238-87b03a657514"
      },
      "source": [
        "d3qn.epsilon = 0.0\n",
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action = d3qn.choose_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "id": "6lio08_Iqg5w",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 44.0\n",
            "Episode: 1 score: 51.0\n",
            "Episode: 2 score: 61.0\n",
            "Episode: 3 score: 77.0\n",
            "Episode: 4 score: 59.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7SAiEb8f0p5"
      },
      "source": [
        "#train_dir = './trained_models/CartPole/DuelingDeepQ'\n",
        "train_dir = '.'\n",
        "d3qn.epsilon = 0.0\n",
        "d3qn.save_model(train_dir)"
      ],
      "id": "c7SAiEb8f0p5",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrl2q5tlkUUh"
      },
      "source": [
        "## Test"
      ],
      "id": "jrl2q5tlkUUh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPpN4cwlkXwa"
      },
      "source": [
        "trained_d3qn = Agent(lr=0.005, gamma=0.99, n_actions=env.action_space.n, epsilon=0.0, batch_size=64, input_dims=env.observation_space.shape)\n",
        "trained_d3qn.load_model(train_dir, learned_steps=469)"
      ],
      "id": "VPpN4cwlkXwa",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oDodUSoklqc",
        "outputId": "98cdc035-946b-4375-9c95-6dff8ff0cfce"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action = trained_d3qn.choose_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "id": "3oDodUSoklqc",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 53.0\n",
            "Episode: 1 score: 57.0\n",
            "Episode: 2 score: 58.0\n",
            "Episode: 3 score: 61.0\n",
            "Episode: 4 score: 54.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXrdbHtZk4i7"
      },
      "source": [
        "# From scratch - Deep Q - Keras"
      ],
      "id": "kXrdbHtZk4i7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6QBn9Juk4jW"
      },
      "source": [
        "## Replay Buffer"
      ],
      "id": "h6QBn9Juk4jW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwFZSOHjk4jW"
      },
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, max_size, input_shape):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "    \n",
        "    self.states = np.zeros((self.mem_size, *input_shape), dtype=np.float64)\n",
        "    self.next_states = np.zeros((self.mem_size, *input_shape), dtype=np.float64)\n",
        "    self.rewards = np.zeros(self.mem_size, dtype=np.float64)\n",
        "    self.actions = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.done = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "  def store_step(self, state, action, reward, next_state, done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.states[index] = state\n",
        "    self.next_states[index] = next_state\n",
        "    self.actions[index] = action\n",
        "    self.rewards[index] = reward\n",
        "    self.done[index] = done\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_counter, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "    states = self.states[batch]\n",
        "    next_states = self.next_states[batch]\n",
        "    rewards = self.rewards[batch]\n",
        "    actions = self.actions[batch]\n",
        "    done = self.done[batch]\n",
        "\n",
        "    return states, actions, rewards, next_states, done\n",
        "    "
      ],
      "id": "rwFZSOHjk4jW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmWnmqZAk4jX"
      },
      "source": [
        "## Q Network"
      ],
      "id": "wmWnmqZAk4jX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T1jPO9Zk4jX"
      },
      "source": [
        "class DeepQNetwork(keras.Model):\n",
        "  def __init__(self, n_actions):\n",
        "    super(DeepQNetwork, self).__init__()\n",
        "\n",
        "    fc1_dims = 128\n",
        "    fc2_dims = 128\n",
        "    self.dense1 = keras.layers.Dense(fc1_dims, activation='relu')\n",
        "    self.dense2 = keras.layers.Dense(fc2_dims, activation='relu')    \n",
        "    self.Q = keras.layers.Dense(n_actions, activation=None)\n",
        "\n",
        "  def call(self, state):\n",
        "    x = self.dense1(state)\n",
        "    x = self.dense2(x)\n",
        "    Q = self.Q(x)    \n",
        "    return Q"
      ],
      "id": "7T1jPO9Zk4jX",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFgPkRBMk4jX"
      },
      "source": [
        "## Agent"
      ],
      "id": "HFgPkRBMk4jX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4CAzO0Wk4jX"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, lr, gamma, n_actions, epsilon, batch_size, input_dims, epsilon_dec=1e-3, epsilon_end=0.01, mem_size=1000000, replace=100):\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.gamma =gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_dec = epsilon_dec\n",
        "    self.epsilon_end = epsilon_end\n",
        "    self.replace = replace\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.learned_step_counter = 0\n",
        "    self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "    self.q =  DeepQNetwork(n_actions)\n",
        "\n",
        "    self.q.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "  def store_step(self, state, action, reward, next_state, done):\n",
        "    self.memory.store_step(state, action, reward, next_state, done)\n",
        "  \n",
        "  def choose_action(self, observation):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action_space)\n",
        "    else:\n",
        "      state = np.array([observation])\n",
        "      actions = self.q(state)\n",
        "      action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "\n",
        "    # get data\n",
        "    states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "    q_pred = self.q(states)\n",
        "    q_next = self.q(next_states)\n",
        "    q_target = q_pred.numpy()\n",
        "    max_next_actions = tf.math.argmax(q_next, axis=1)\n",
        "    for i, terminated in enumerate(dones):\n",
        "      q_target[i, actions[i]] = rewards[i] + self.gamma*q_next[i, max_next_actions[i]]*(1-int(dones[i]))\n",
        "\n",
        "    # train\n",
        "    self.q.train_on_batch(states, q_target)\n",
        "\n",
        "    self.epsilon = max(self.epsilon - self.epsilon_dec, self.epsilon_end)\n",
        "    self.learned_step_counter += 1\n",
        "\n",
        "  def train(self, env, n_games):\n",
        "    scores = []\n",
        "    eps_history = []\n",
        "    steps = 0\n",
        "    for i in range(n_games):\n",
        "      done = False\n",
        "      score = 0\n",
        "      observation = env.reset()\n",
        "      while not done:\n",
        "        steps += 1\n",
        "        action = self.choose_action(observation)\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        self.store_step(observation, action, reward, next_observation, done)\n",
        "        observation = next_observation\n",
        "        self.learn()\n",
        "      eps_history.append(self.epsilon)\n",
        "      scores.append(score)\n",
        "      avg_score = np.mean(scores[-10:])\n",
        "      print('Episode', i, '- trained steps', steps, '- score %.1f'%score, '- avg_score %.1f ' % avg_score)\n",
        "\n",
        "  def save_model(self, train_dir):\n",
        "    file_name = train_dir + '/dqn_' + str(self.learned_step_counter) + '/model'\n",
        "    self.q.save_weights(file_name, save_format='tf')\n",
        "\n",
        "  def load_model(self, train_dir, learned_steps = 100):\n",
        "    file_name = train_dir + '/dqn_' + str(learned_steps) + '/model' \n",
        "    self.q.load_weights(file_name)\n"
      ],
      "id": "c4CAzO0Wk4jX",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBdHiy7k4jY"
      },
      "source": [
        "## Train"
      ],
      "id": "HtBdHiy7k4jY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvbGMgZk4jY"
      },
      "source": [
        "dqn = Agent(lr=0.005, gamma=0.99, n_actions=env.action_space.n, epsilon=1.0, batch_size=64, input_dims=env.observation_space.shape)"
      ],
      "id": "jGvbGMgZk4jY",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cGuRrulkk4jY",
        "outputId": "d000d9b6-9c2c-442a-9866-ba812e90799b"
      },
      "source": [
        "n_games = 100\n",
        "dqn.train(env, n_games)"
      ],
      "id": "cGuRrulkk4jY",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0 - trained steps 21 - score 21.0 - avg_score 21.0 \n",
            "Episode 1 - trained steps 45 - score 24.0 - avg_score 22.5 \n",
            "Episode 2 - trained steps 63 - score 18.0 - avg_score 21.0 \n",
            "Episode 3 - trained steps 135 - score 72.0 - avg_score 33.8 \n",
            "Episode 4 - trained steps 147 - score 12.0 - avg_score 29.4 \n",
            "Episode 5 - trained steps 172 - score 25.0 - avg_score 28.7 \n",
            "Episode 6 - trained steps 183 - score 11.0 - avg_score 26.1 \n",
            "Episode 7 - trained steps 193 - score 10.0 - avg_score 24.1 \n",
            "Episode 8 - trained steps 213 - score 20.0 - avg_score 23.7 \n",
            "Episode 9 - trained steps 232 - score 19.0 - avg_score 23.2 \n",
            "Episode 10 - trained steps 256 - score 24.0 - avg_score 23.5 \n",
            "Episode 11 - trained steps 279 - score 23.0 - avg_score 23.4 \n",
            "Episode 12 - trained steps 304 - score 25.0 - avg_score 24.1 \n",
            "Episode 13 - trained steps 314 - score 10.0 - avg_score 17.9 \n",
            "Episode 14 - trained steps 325 - score 11.0 - avg_score 17.8 \n",
            "Episode 15 - trained steps 341 - score 16.0 - avg_score 16.9 \n",
            "Episode 16 - trained steps 358 - score 17.0 - avg_score 17.5 \n",
            "Episode 17 - trained steps 369 - score 11.0 - avg_score 17.6 \n",
            "Episode 18 - trained steps 381 - score 12.0 - avg_score 16.8 \n",
            "Episode 19 - trained steps 405 - score 24.0 - avg_score 17.3 \n",
            "Episode 20 - trained steps 444 - score 39.0 - avg_score 18.8 \n",
            "Episode 21 - trained steps 483 - score 39.0 - avg_score 20.4 \n",
            "Episode 22 - trained steps 551 - score 68.0 - avg_score 24.7 \n",
            "Episode 23 - trained steps 591 - score 40.0 - avg_score 27.7 \n",
            "Episode 24 - trained steps 645 - score 54.0 - avg_score 32.0 \n",
            "Episode 25 - trained steps 724 - score 79.0 - avg_score 38.3 \n",
            "Episode 26 - trained steps 776 - score 52.0 - avg_score 41.8 \n",
            "Episode 27 - trained steps 832 - score 56.0 - avg_score 46.3 \n",
            "Episode 28 - trained steps 866 - score 34.0 - avg_score 48.5 \n",
            "Episode 29 - trained steps 908 - score 42.0 - avg_score 50.3 \n",
            "Episode 30 - trained steps 957 - score 49.0 - avg_score 51.3 \n",
            "Episode 31 - trained steps 1016 - score 59.0 - avg_score 53.3 \n",
            "Episode 32 - trained steps 1051 - score 35.0 - avg_score 50.0 \n",
            "Episode 33 - trained steps 1087 - score 36.0 - avg_score 49.6 \n",
            "Episode 34 - trained steps 1169 - score 82.0 - avg_score 52.4 \n",
            "Episode 35 - trained steps 1224 - score 55.0 - avg_score 50.0 \n",
            "Episode 36 - trained steps 1254 - score 30.0 - avg_score 47.8 \n",
            "Episode 37 - trained steps 1341 - score 87.0 - avg_score 50.9 \n",
            "Episode 38 - trained steps 1380 - score 39.0 - avg_score 51.4 \n",
            "Episode 39 - trained steps 1420 - score 40.0 - avg_score 51.2 \n",
            "Episode 40 - trained steps 1493 - score 73.0 - avg_score 53.6 \n",
            "Episode 41 - trained steps 1537 - score 44.0 - avg_score 52.1 \n",
            "Episode 42 - trained steps 1587 - score 50.0 - avg_score 53.6 \n",
            "Episode 43 - trained steps 1623 - score 36.0 - avg_score 53.6 \n",
            "Episode 44 - trained steps 1692 - score 69.0 - avg_score 52.3 \n",
            "Episode 45 - trained steps 1773 - score 81.0 - avg_score 54.9 \n",
            "Episode 46 - trained steps 1881 - score 108.0 - avg_score 62.7 \n",
            "Episode 47 - trained steps 1997 - score 116.0 - avg_score 65.6 \n",
            "Episode 48 - trained steps 2192 - score 195.0 - avg_score 81.2 \n",
            "Episode 49 - trained steps 2311 - score 119.0 - avg_score 89.1 \n",
            "Episode 50 - trained steps 2498 - score 187.0 - avg_score 100.5 \n",
            "Episode 51 - trained steps 2649 - score 151.0 - avg_score 111.2 \n",
            "Episode 52 - trained steps 2832 - score 183.0 - avg_score 124.5 \n",
            "Episode 53 - trained steps 2982 - score 150.0 - avg_score 135.9 \n",
            "Episode 54 - trained steps 3182 - score 200.0 - avg_score 149.0 \n",
            "Episode 55 - trained steps 3382 - score 200.0 - avg_score 160.9 \n",
            "Episode 56 - trained steps 3582 - score 200.0 - avg_score 170.1 \n",
            "Episode 57 - trained steps 3782 - score 200.0 - avg_score 178.5 \n",
            "Episode 58 - trained steps 3982 - score 200.0 - avg_score 179.0 \n",
            "Episode 59 - trained steps 4182 - score 200.0 - avg_score 187.1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d23208cb1d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-d168c9d5f79f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, n_games)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       \u001b[0meps_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-d168c9d5f79f>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1815\u001b[0m     \"\"\"\n\u001b[1;32m   1816\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_on_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_on_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_call_args\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m       \u001b[0mpositional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullargspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2587\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'training'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositional_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2588\u001b[0m       \u001b[0mpositional_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSTiMi4tk4jY",
        "outputId": "03521a8f-49fe-4a98-bfc3-1effd5efd149"
      },
      "source": [
        "dqn.epsilon = 0.0\n",
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action = dqn.choose_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "id": "pSTiMi4tk4jY",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 200.0\n",
            "Episode: 1 score: 200.0\n",
            "Episode: 2 score: 200.0\n",
            "Episode: 3 score: 200.0\n",
            "Episode: 4 score: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnpYyUrk4jZ"
      },
      "source": [
        "train_dir = '.'\n",
        "dqn.epsilon = 0.0\n",
        "dqn.save_model(train_dir)"
      ],
      "id": "xmnpYyUrk4jZ",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndK62rGYk4jZ"
      },
      "source": [
        "## Test"
      ],
      "id": "ndK62rGYk4jZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHQw5vkWk4ja"
      },
      "source": [
        "trained_dqn = Agent(lr=0.005, gamma=0.99, n_actions=env.action_space.n, epsilon=0.0, batch_size=64, input_dims=env.observation_space.shape)\n",
        "trained_dqn.load_model(train_dir, learned_steps=4241)"
      ],
      "id": "nHQw5vkWk4ja",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThYspHG8k4ja",
        "outputId": "cfc839b7-7cd1-4886-bb0a-630b591eaad8"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action = trained_dqn.choose_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #env.render()\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "    print('Episode: {} score: {}'.format(episode, score))"
      ],
      "id": "ThYspHG8k4ja",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 score: 200.0\n",
            "Episode: 1 score: 200.0\n",
            "Episode: 2 score: 200.0\n",
            "Episode: 3 score: 200.0\n",
            "Episode: 4 score: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2k3UB4YnmT_"
      },
      "source": [
        ""
      ],
      "id": "A2k3UB4YnmT_",
      "execution_count": null,
      "outputs": []
    }
  ]
}